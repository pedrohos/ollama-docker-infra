model_list:
  - model_name: llama3.1:8b-instruct-q6_K
    litellm_params:
      model: ollama_chat/llama3.1:8b-instruct-q6_K
      api_base: http://ollama:11434
      rpm: 3      # Rate limit for this deployment: in requests per minute (rpm)
  - model_name: llama3.1:8b-instruct-q6_K
    litellm_params:
      model: ollama_chat/llama3.1:8b-instruct-q6_K
      api_base: http://ollama-2:11434
      rpm: 3

router_settings:
  routing_strategy: simple-shuffle # Literal["simple-shuffle", "least-busy", "usage-based-routing","latency-based-routing"], default="simple-shuffle"
  # model_group_alias: {"gpt-4": "gpt-3.5-turbo"} # all requests with `gpt-4` will be routed to models with `gpt-3.5-turbo`
  num_retries: 2
  timeout: 1000                                  # 30 seconds
  # redis_host: <your redis host>                # set this when using multiple litellm proxy deployments, load balancing state stored in redis
  # redis_password: <your redis password>
  # redis_port: 1992